# ğŸ¯ Graduate Underemployment / Overqualification Prediction â€” ML Pipeline

This repository implements a **full machine learning pipeline** for predicting **overqualification** (underemployment) in recruitment using the **NGS (National Graduate Survey) structured hiring dataset**. It was developed in the context of the **ML Hackathon hosted by the SFU Data Science Student Society**, where teams worked with real-world datasets to design, train, and evaluate predictive models in a competitive setting.

The pipeline uses **CatBoost** as the primary model, with a focus on **predictive performance** (accuracy on Public/Private leaderboards) and **interpretability** (feature importance and optional SHAP).

---

## ğŸ¯ Project Overview

The goal of this project is to:

- **Build a robust model** that accurately estimates overqualification probability based on candidate attributes: education level, years of experience, skill composition, prior roles, and demographics.
- **Work with the NGS dataset** and understand its feature structure (survey codes, missing conventions, mixed-type columns).
- **Train and tune a CatBoost-based** machine learning model with validation feedback and leaderboard-oriented iteration.
- **Focus on both predictive performance and interpretability** â€” accuracy on hold-out test sets and feature importance / SHAP-style explanations.

The solution achieved **0.75174** accuracy on the Public leaderboard and **0.70511** on the Private leaderboard, placing it very close to the top-performing teams and demonstrating strong generalization on unseen data.

---

## âœ¨ Key Features

- **Modular ML pipeline** (`src/` folder): clean separation of data loading, preprocessing, feature engineering, model training, evaluation, and prediction.
- **NGS-aware preprocessing:** handling of special codes (6, 9, 99) and normalization of mixed-type columns (e.g. GENDER2, DDIS_FL, VISBMINP).
- **CatBoost classifier** with native categorical support, early stopping, and configurable hyperparameters (depth, learning_rate, l2_leaf_reg).
- **Stratified K-fold cross-validation** and optional grid search for hyperparameter tuning.
- **Interpretability:** CatBoost feature importance and optional SHAP integration for model explanation.
- **Reproducible workflow:** `python3 -m src.train` and `python3 -m src.predict` for end-to-end training and submission generation.
- **Five structured Jupyter notebooks** documenting exploration, preprocessing, training/tuning, evaluation/interpretability, and the full pipeline demo.

---

## ğŸ§± Repository Structure

```
graduate-underemployment-prediction/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/                                  # Processed/cached data (optional); not in Git
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ train.csv                               # Training set (id, features, overqualified)
â”‚       â””â”€â”€ test.csv                                # Test set (id, features; no target)
â”‚
â”œâ”€â”€ models/                                         # Saved model artifacts (model.cbm, artifacts.pkl); not in Git
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploration.ipynb                        # EDA, NGS feature structure, target and correlations
â”‚   â”œâ”€â”€ 02_preprocessing_feature_engineering.ipynb  # Cleaning and categorical encoding
â”‚   â”œâ”€â”€ 03_catboost_training_tuning.ipynb           # Training, CV, hyperparameter tuning
â”‚   â”œâ”€â”€ 04_evaluation_interpretability.ipynb        # Metrics, feature importance, SHAP
â”‚   â””â”€â”€ 05_pipeline_demo.ipynb                      # End-to-end pipeline demonstration
â”‚
â”œâ”€â”€ submissions/                                    # Generated submission CSVs (id, overqualified)
â”‚   â””â”€â”€ submission.csv                              # Default output from python3 -m src.predict
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                                   # Paths, target/id columns, validation settings
â”‚   â”œâ”€â”€ data.py                                     # Load train/test, split X/y, train/val split
â”‚   â”œâ”€â”€ evaluate.py                                 # Stratified K-fold CV and accuracy
â”‚   â”œâ”€â”€ features.py                                 # Categorical feature preparation for CatBoost
â”‚   â”œâ”€â”€ hyperparameter_tuning.py                    # Grid search for CatBoost params
â”‚   â”œâ”€â”€ model.py                                    # CatBoost classifier builder
â”‚   â”œâ”€â”€ preprocess.py                               # NGS cleaning and categorical normalization
â”‚   â”œâ”€â”€ predict.py                                  # Load model, predict on test, write submission
â”‚   â””â”€â”€ train.py                                    # End-to-end training pipeline
â”‚
â”œâ”€â”€ .gitignore                                      # Git ignore rules (venv, models/*, cache, etc.)
â”œâ”€â”€ LICENSE                                         # MIT license
â”œâ”€â”€ README.md                                       # Project overview and usage
â”œâ”€â”€ report.md                                       # Detailed technical write-up
â””â”€â”€ requirements.txt                                # Python dependencies
```

> ğŸ—’ï¸ **Note:**  
> The `data/raw/` directory should contain `train.csv` and `test.csv`. The `models/` directory is where the trained CatBoost model and artifacts are saved after running `python3 -m src.train`, **`models/` is not tracked in Git** (it is in `.gitignore`), so you need to run the training pipeline locally to generate the model. Processed data is not stored on disk; all transformations are applied in memory during training and prediction.

---

## ğŸ§° Run Locally

You can run this project on your machine using **Python 3.11+** and a virtual environment.

### 1ï¸âƒ£ Clone the repository

**HTTPS (recommended for most users):**
```bash
git clone https://github.com/florykhan/graduate-underemployment-prediction.git
cd graduate-underemployment-prediction
```

**SSH (for users who have SSH keys configured):**
```bash
git clone git@github.com:florykhan/graduate-underemployment-prediction.git
cd graduate-underemployment-prediction
```

### 2ï¸âƒ£ Create and activate a virtual environment

```bash
python3 -m venv venv
source venv/bin/activate      # macOS/Linux
venv\Scripts\activate         # Windows
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Add the dataset

Place the NGS hackathon data files in `data/raw/`:

```
data/raw/train.csv   # Training set (must include column: overqualified)
data/raw/test.csv    # Test set (same features, no target)
```

> ğŸ“¥ **Dataset:** The NGS structured hiring dataset was provided as part of the SFU Data Science Student Society ML Hackathon. Ensure `train.csv` has an `id` column and an `overqualified` (0/1) target column; `test.csv` should have the same feature columns and `id`.

### 5ï¸âƒ£ Run the training pipeline

This step trains the CatBoost model, runs validation (and optional CV), and saves the model and artifacts.

```bash
python3 -m src.train
```

### 6ï¸âƒ£ Generate predictions and submission

```bash
python3 -m src.predict
```

This writes `submissions/submission.csv` with columns `id` and `overqualified` (0/1 predictions).

### 7ï¸âƒ£ Run the notebooks

Launch Jupyter and open the notebooks from the project root (so that `notebooks/` is the working directory for paths):

```bash
jupyter notebook
```

Recommended order:

- `notebooks/01_exploration.ipynb` â€” data exploration and NGS feature structure
- `notebooks/02_preprocessing_feature_engineering.ipynb` â€” cleaning and categorical encoding
- `notebooks/03_catboost_training_tuning.ipynb` â€” CatBoost training, CV, and tuning
- `notebooks/04_evaluation_interpretability.ipynb` â€” metrics, feature importance, SHAP
- `notebooks/05_pipeline_demo.ipynb` â€” end-to-end pipeline demo

> **Tip:** If you run notebooks from inside `notebooks/`, the code uses `sys.path.insert(0, str(Path().resolve().parent))` so that `src` can be imported correctly.

---

## ğŸ“Š Results (Summary)

| **Metric** | **Value** |
|------------|-----------|
| Public leaderboard accuracy | **0.75174** (best: 0.76623) |
| Private leaderboard accuracy | **0.70511** (best: 0.71304) |

The tuned CatBoost model placed the solution very close to the top-performing teams and demonstrated strong generalization on the private hold-out set. Validation and cross-validation accuracy (e.g. ~0.67â€“0.75 depending on split and hyperparameters) are used during development; the leaderboard metrics above reflect the official hackathon evaluation.

â¡ï¸ For methodology, preprocessing details, model choices, and full discussion, see: [`report.md`](report.md).

---

## ğŸ“„ Full Technical Report

The complete technical write-up, including pipeline design, preprocessing and feature engineering, CatBoost training and tuning, validation strategy, and interpretability, is in [`report.md`](report.md). This document is intended for reviewers who want the full methodology behind the pipeline and results.

---

## ğŸš€ Future Directions

- **Expand hyperparameter search:** use RandomizedSearchCV or Optuna over a larger CatBoost parameter space.
- **Feature engineering:** additional derived features (e.g. educationâ€“occupation match indicators) if metadata is available.
- **Ensembles:** combine CatBoost with other classifiers (e.g. XGBoost, LightGBM) for potential accuracy gains.
- **Experiment tracking:** integrate MLflow or Weights & Biases to log metrics and compare runs.
- **Production readiness:** API (FastAPI/Flask), Docker, or CI/CD for training and deployment.

---

## ğŸ§  Tech Stack

- **Language:** Python 3.11+
- **Core libraries:** pandas, numpy, scikit-learn, CatBoost, matplotlib, seaborn
- **Pipeline:** Modular `src/` package with config, data loading, preprocessing, feature engineering, model, evaluation, tuning, train, and predict
- **Environment:** Jupyter Notebook / VS Code; Git

---

## ğŸ§¾ License

MIT License, feel free to use and modify with attribution. See the [`LICENSE`](LICENSE) file for full details.

---

## ğŸ‘¤ Authors

**Ilian Khankhalaev**  
_BSc Computing Science, Simon Fraser University_  
ğŸ“ Vancouver, BC  |  [florykhan@gmail.com](mailto:florykhan@gmail.com)  |  [GitHub](https://github.com/florykhan)  |  [LinkedIn](https://www.linkedin.com/in/ilian-khankhalaev/)

**Nikolay Deinego**  
_BSc Computing Science, Simon Fraser University_  
ğŸ“ Vancouver, BC  | [GitHub](https://github.com/Deinick)  |  [LinkedIn](https://www.linkedin.com/in/nikolay-deinego/)

**Anna Cherkashina**
_BSc Data Science, Simon Fraser University_  
ğŸ“ Vancouver, BC  | [GitHub](https://github.com/Anna05072005)  |  [LinkedIn](https://www.linkedin.com/in/anna-cherkashina-467059293/)

**Arina Veprikova**  
_BSc Data Science, Simon Fraser University_  
ğŸ“ Vancouver, BC  |  [GitHub](https://github.com/areenve)  |  [LinkedIn](https://www.linkedin.com/in/arina-veprikova-a97526366/)
