{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# 03 ‚Äî CatBoost Training & Tuning\n",
    "\n",
    "> **Objective:** To train and tune a CatBoost classifier for overqualification prediction, using validation feedback and (optionally) grid search over key hyperparameters to approach leaderboard-level accuracy.\n",
    "\n",
    "This notebook covers:\n",
    "1. [**Data preparation**](#data-preparation) ‚Äî load, clean, split, and define categorical indices  \n",
    "2. [**Baseline training**](#baseline-training) ‚Äî single train/val split and full fit  \n",
    "3. [**Cross-validation**](#cross-validation) ‚Äî stratified K-fold and mean accuracy  \n",
    "4. [**Hyperparameter tuning**](#hyperparameter-tuning) ‚Äî grid search over depth, learning_rate, l2_leaf_reg  \n",
    "5. [**Final model**](#final-model) ‚Äî retrain on full training set with chosen parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context",
   "metadata": {},
   "source": [
    "### üß† Context\n",
    "\n",
    "The hackathon evaluation metric was **accuracy** on a held-out test set (Public/Private leaderboard). We use **stratified K-fold cross-validation** and a **train/validation split** to estimate generalization and avoid overfitting. CatBoost handles categorical features natively and supports **early stopping** on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "---\n",
    "### üß∞ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sys.path.insert(0, str(Path().resolve().parent))\n",
    "\n",
    "from src.config import VAL_SIZE, RANDOM_STATE, N_FOLDS\n",
    "from src.data import load_train, split_X_y, get_train_val_split\n",
    "from src.preprocess import clean\n",
    "from src.features import add_features, get_categorical_feature_names\n",
    "from src.model import build_model\n",
    "from src.evaluate import run_validation\n",
    "from src.hyperparameter_tuning import grid_search_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-prep",
   "metadata": {},
   "source": [
    "### üì• Data Preparation <a id=\"data-preparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prep-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (7709, 23)\n",
      "Categorical feature indices: [0, 1, 2, 3, 4] ...\n",
      "Target distribution: {0: 4745, 1: 2964}\n"
     ]
    }
   ],
   "source": [
    "df = load_train()\n",
    "df = clean(df)\n",
    "df = add_features(df)\n",
    "\n",
    "X, y = split_X_y(df, target_col=\"overqualified\")\n",
    "y = y.astype(int)\n",
    "\n",
    "cat_names = [c for c in get_categorical_feature_names() if c in X.columns]\n",
    "cat_indices = [i for i, c in enumerate(X.columns) if c in cat_names]\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Categorical feature indices:\", cat_indices[:5], \"...\")\n",
    "print(\"Target distribution:\", y.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline",
   "metadata": {},
   "source": [
    "### üèÉ Baseline Training <a id=\"baseline-training\"></a>\n",
    "\n",
    "Single train/validation split; CatBoost with early stopping on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baseline-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6246149\ttest: 0.6115435\tbest: 0.6115435 (0)\ttotal: 64.7ms\tremaining: 32.3s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.6640726329\n",
      "bestIteration = 9\n",
      "\n",
      "Shrink model to first 10 iterations.\n",
      "Validation accuracy: 0.6641\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = get_train_val_split(df, val_size=VAL_SIZE, random_state=RANDOM_STATE)\n",
    "X_train, y_train = split_X_y(train_df, target_col=\"overqualified\")\n",
    "X_val, y_val = split_X_y(val_df, target_col=\"overqualified\")\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)\n",
    "\n",
    "model = build_model(iterations=500, learning_rate=0.05, depth=6, early_stopping_rounds=20)\n",
    "model.fit(X_train, y_train, cat_features=cat_indices, eval_set=(X_val, y_val))\n",
    "\n",
    "val_pred = model.predict(X_val)\n",
    "print(\"Validation accuracy:\", round(accuracy_score(y_val, val_pred), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cv",
   "metadata": {},
   "source": [
    "### üìä Cross-Validation <a id=\"cross-validation\"></a>\n",
    "\n",
    "Stratified 5-fold CV to estimate mean accuracy and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cv-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.6640 ¬± 0.0053\n",
      "Fold accuracies: [0.6719, 0.6608, 0.6608, 0.6686, 0.658]\n"
     ]
    }
   ],
   "source": [
    "cv_results = run_validation(\n",
    "    X, y,\n",
    "    model=None,\n",
    "    n_folds=N_FOLDS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    cat_indices=cat_indices,\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    early_stopping_rounds=20,\n",
    ")\n",
    "print(f\"CV accuracy: {cv_results['mean_accuracy']:.4f} ¬± {cv_results['std_accuracy']:.4f}\")\n",
    "print(\"Fold accuracies:\", [round(a, 4) for a in cv_results[\"fold_accuracies\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning",
   "metadata": {},
   "source": [
    "### üîß Hyperparameter Tuning <a id=\"hyperparameter-tuning\"></a>\n",
    "\n",
    "Optional grid search over `depth`, `learning_rate`, and `l2_leaf_reg`. (Small grid to keep runtime reasonable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tuning-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'depth': 7, 'learning_rate': 0.08, 'l2_leaf_reg': 5.0}\n",
      "Best CV accuracy: 0.6771\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"depth\": [5, 6, 7],\n",
    "    \"learning_rate\": [0.03, 0.05, 0.08],\n",
    "    \"l2_leaf_reg\": [2.0, 3.0, 5.0],\n",
    "}\n",
    "\n",
    "# Reduce grid or n_folds for faster run (e.g. 2 folds, 1 value per param for a quick test)\n",
    "tuning_results = grid_search_cv(\n",
    "    X, y,\n",
    "    param_grid=param_grid,\n",
    "    cat_indices=cat_indices,\n",
    "    n_folds=3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    early_stopping_rounds=15,\n",
    ")\n",
    "\n",
    "best = max(tuning_results, key=lambda x: x[\"mean_accuracy\"])\n",
    "print(\"Best params:\", best[\"params\"])\n",
    "print(\"Best CV accuracy:\", round(best[\"mean_accuracy\"], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final",
   "metadata": {},
   "source": [
    "### ‚úÖ Final Model <a id=\"final-model\"></a>\n",
    "\n",
    "Retrain on the **full** training set with the chosen hyperparameters (or defaults). The same logic is run by `python3 -m src.train` from the project root, which also saves the model and artifacts for `predict.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "final-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6488520\ttotal: 9.3ms\tremaining: 4.64s\n",
      "100:\tlearn: 0.7314827\ttotal: 1.2s\tremaining: 4.73s\n",
      "200:\tlearn: 0.7666364\ttotal: 2.36s\tremaining: 3.51s\n",
      "300:\tlearn: 0.8002335\ttotal: 3.44s\tremaining: 2.27s\n",
      "400:\tlearn: 0.8264366\ttotal: 4.47s\tremaining: 1.1s\n",
      "499:\tlearn: 0.8516020\ttotal: 5.5s\tremaining: 0us\n",
      "Final model trained on full training set.\n",
      "To save and generate submission: run from terminal: python3 -m src.train && python3 -m src.predict\n"
     ]
    }
   ],
   "source": [
    "# Use best params from tuning if available; otherwise defaults\n",
    "try:\n",
    "    final_params = best[\"params\"]\n",
    "except NameError:\n",
    "    final_params = {\"depth\": 6, \"learning_rate\": 0.05, \"l2_leaf_reg\": 3.0}\n",
    "\n",
    "final_model = build_model(\n",
    "    iterations=500,\n",
    "    early_stopping_rounds=20,\n",
    "    random_seed=RANDOM_STATE,\n",
    "    **final_params,\n",
    ")\n",
    "final_model.fit(X, y, cat_features=cat_indices)\n",
    "\n",
    "print(\"Final model trained on full training set.\")\n",
    "print(\"To save and generate submission: run from terminal: python3 -m src.train && python3 -m src.predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Summary\n",
    "\n",
    "We trained a CatBoost classifier with early stopping and optional grid search. Validation and CV accuracy guide parameter choice; the production pipeline in `src/train.py` retrains on the full training data and saves the model for submission generation.\n",
    "\n",
    "**Next step:** `04_evaluation_interpretability.ipynb` ‚Äî feature importance and model interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
