{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# 05 ‚Äî End-to-End Pipeline Demo\n",
    "\n",
    "> **Objective:** To demonstrate the full ML pipeline for overqualification prediction: from raw data to trained CatBoost model and submission file, matching the workflow used in the SFU Data Science ML Hackathon.\n",
    "\n",
    "This notebook:\n",
    "1. [**Loads and preprocesses**](#step-1-load-and-preprocess) the NGS training data  \n",
    "2. [**Trains**](#step-2-train) the CatBoost model with validation  \n",
    "3. [**Generates predictions**](#step-3-predict) on the test set and builds a submission DataFrame  \n",
    "4. [**Recaps**](#summary) the pipeline and how to run it from the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context",
   "metadata": {},
   "source": [
    "### üß† Context\n",
    "\n",
    "The production pipeline is implemented in **`src/`**: `data`, `preprocess`, `features`, `model`, `train`, `evaluate`, `predict`. This notebook replays the same steps in order so you can see the full flow in one place. For reproducible training and submission generation, run from the project root:\n",
    "\n",
    "```bash\n",
    "python3 -m src.train\n",
    "python3 -m src.predict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports",
   "metadata": {},
   "source": [
    "---\n",
    "### üß∞ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, str(Path().resolve().parent))\n",
    "\n",
    "from src.config import TRAIN_CSV, MODEL_ARTIFACT_DIR, SUBMISSIONS_DIR\n",
    "from src.data import load_train, load_test, split_X_y, get_train_val_split\n",
    "from src.preprocess import clean\n",
    "from src.features import add_features, get_categorical_feature_names\n",
    "from src.model import build_model\n",
    "from src.train import run_train_pipeline\n",
    "from src.predict import run_predict_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "### Step 1: Load and Preprocess <a id=\"step-1-load-and-preprocess\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "step1-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 7709\n",
      "Features: ['CERTLEVP', 'PGMCIPAP', 'PGM_P034', 'PGM_P036', 'PGM_280A', 'PGM_280B', 'PGM_280C', 'PGM_280F', 'PGM_P401', 'STULOANS', 'DBTOTGRD', 'SCHOLARP', 'PREVLEVP', 'HLOSGRDP', 'GRADAGEP', 'GENDER2', 'CTZSHIPP', 'VISBMINP', 'DDIS_FL', 'PAR1GRD', 'PAR2GRD', 'BEF_P140', 'BEF_160']\n",
      "Target distribution: {0: 4745, 1: 2964}\n"
     ]
    }
   ],
   "source": [
    "df_train = load_train()\n",
    "df_train = clean(df_train)\n",
    "df_train = add_features(df_train)\n",
    "\n",
    "X, y = split_X_y(df_train, target_col=\"overqualified\")\n",
    "y = y.astype(int)\n",
    "\n",
    "print(\"Training samples:\", len(df_train))\n",
    "print(\"Features:\", list(X.columns))\n",
    "print(\"Target distribution:\", y.value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "### Step 2: Train <a id=\"step-2-train\"></a>\n",
    "\n",
    "Run the full training pipeline (validation + retrain on full train, save model and artifacts). This may take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "step2-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6246149\ttest: 0.6115435\tbest: 0.6115435 (0)\ttotal: 2.48ms\tremaining: 1.24s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.6640726329\n",
      "bestIteration = 9\n",
      "\n",
      "Shrink model to first 10 iterations.\n",
      "Validation accuracy: 0.6641\n",
      "CV accuracy: 0.6669 ¬± 0.0081\n",
      "0:\tlearn: 0.6488520\ttotal: 6.84ms\tremaining: 3.41s\n",
      "100:\tlearn: 0.6982747\ttotal: 729ms\tremaining: 2.88s\n",
      "200:\tlearn: 0.7198080\ttotal: 1.71s\tremaining: 2.54s\n",
      "300:\tlearn: 0.7369309\ttotal: 2.55s\tremaining: 1.69s\n",
      "400:\tlearn: 0.7505513\ttotal: 3.37s\tremaining: 832ms\n",
      "499:\tlearn: 0.7636529\ttotal: 4.2s\tremaining: 0us\n",
      "Model saved to /Users/florykhan/Documents/Projects/Data & Machine Learning/graduate-underemployment-prediction/models/model.cbm\n",
      "\n",
      "Metrics: {'cv_mean_accuracy': 0.6669394198703666, 'cv_std_accuracy': 0.008124213340195165, 'val_accuracy': 0.6640726329442282}\n"
     ]
    }
   ],
   "source": [
    "metrics = run_train_pipeline(validate=True)\n",
    "print(\"\\nMetrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3",
   "metadata": {},
   "source": [
    "### Step 3: Predict <a id=\"step-3-predict\"></a>\n",
    "\n",
    "Load the saved model, run prediction on the test set, and write the submission CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "step3-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission written to /Users/florykhan/Documents/Projects/Data & Machine Learning/graduate-underemployment-prediction/submissions/submission.csv (2508 rows)\n",
      "Submission preview:\n",
      "      id  overqualified\n",
      "0  10636              0\n",
      "1  12024              0\n",
      "2  11353              1\n",
      "3  10555              0\n",
      "4   7751              1\n",
      "5   7003              0\n",
      "6    807              0\n",
      "7   7413              0\n",
      "8   5121              1\n",
      "9   3817              0\n",
      "\n",
      "Submission shape: (2508, 2)\n"
     ]
    }
   ],
   "source": [
    "out_path = run_predict_pipeline(output_name=\"submission.csv\")\n",
    "sub = pd.read_csv(out_path)\n",
    "print(\"Submission preview:\")\n",
    "print(sub.head(10))\n",
    "print(\"\\nSubmission shape:\", sub.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Summary <a id=\"summary\"></a>\n",
    "\n",
    "The end-to-end pipeline:\n",
    "\n",
    "1. **Data** ‚Äî `data/raw/train.csv`, `data/raw/test.csv`  \n",
    "2. **Preprocessing** ‚Äî `clean()` (NGS codes, mixed types) and `add_features()` (categorical strings)  \n",
    "3. **Training** ‚Äî CatBoost with early stopping; optional CV and hyperparameter tuning  \n",
    "4. **Artifacts** ‚Äî `models/model.cbm`, `models/artifacts.pkl`  \n",
    "5. **Submission** ‚Äî `submissions/submission.csv` (id, overqualified)\n",
    "\n",
    "To reproduce from the terminal:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "python3 -m src.train\n",
    "python3 -m src.predict\n",
    "```\n",
    "\n",
    "For full methodology, results, and leaderboard context, see the [README](../README.md) and [reports/report.md](../reports/report.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
